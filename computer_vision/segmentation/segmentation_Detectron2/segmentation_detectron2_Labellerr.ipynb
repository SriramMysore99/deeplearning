{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "segmentation_detectron2_Labellerr.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMgTz0tFM3xf"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJR-5hRnNBVe"
      },
      "source": [
        "#Install Detectron2 gpu on colab\n",
        "\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html\n",
        "exit(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CBhQk-YNBYN"
      },
      "source": [
        "#Import the required libraries\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import random\n",
        "from google.colab.patches import cv2_imshow\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data import DatasetMapper, build_detection_train_loader\n",
        "from detectron2.data import DatasetCatalog\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frgwZ7d_NBa8"
      },
      "source": [
        "#Create the utility functions\n",
        "\n",
        "def get_dataset(JSON_PATH):\n",
        "    \"\"\"[utility function to supply data in list[dict] format for registration in detectron2]\n",
        "\n",
        "    Args:\n",
        "            JSON_PATH ([str]): [path to the json data file]\n",
        "\n",
        "    Returns:\n",
        "            [list[dict]]: [the data format needed for detectron2]\n",
        "    \"\"\"\n",
        "    with open(JSON_PATH) as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "\n",
        "def register_dataset(dataset_name, input_json_path, thing_classes):\n",
        "    \"\"\"[function to register dataset in detectron2 format]\n",
        "\n",
        "    Args:\n",
        "            dataset_name ([str]): [name of the dataset to register]\n",
        "            input_json_path ([str]): [path to the json in detectron2 format]\n",
        "            keypoint_names ([list[str]]): [names of the keypoints to detect]\n",
        "            keypoint_flip_map ([list[tuple(str)]]): [the keypoints which flip the relative position during flip augmentation]\n",
        "            thing_classes ([list[str]]): [thing classes- the class of the bounding boxes]\n",
        "\n",
        "    Returns:\n",
        "            [type]: [metadata]\n",
        "    \"\"\"\n",
        "\n",
        "    DatasetCatalog.register(dataset_name, lambda: get_dataset(input_json_path))\n",
        "    MetadataCatalog.get(dataset_name).set(thing_classes=thing_classes)\n",
        "    metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "    return metadata\n",
        "\n",
        "\n",
        "def create_config(config_dict):\n",
        "    \"\"\"[function to create config file for model training]\n",
        "\n",
        "    Args:\n",
        "            config_dict ([dict{}]): [the configuations to apply]\n",
        "\n",
        "    Returns:\n",
        "            [type]: [config]\n",
        "    \"\"\"\n",
        "\n",
        "    cfg = get_cfg()\n",
        "    cfg.MODEL.DEVICE=config_dict[\"DEVICE\"] # 'cpu' or 'cuda'\n",
        "    cfg.OUTPUT_DIR = config_dict[\"model_out_dir\"]\n",
        "    cfg.merge_from_file(model_zoo.get_config_file(\n",
        "        config_dict[\"model_yaml_file\"]))\n",
        "    cfg.DATASETS.TRAIN = (config_dict[\"train_dataset_name\"],)\n",
        "    cfg.DATASETS.TEST = (config_dict[\"test_dataset_name\"])\n",
        "    cfg.DATALOADER.NUM_WORKERS = config_dict[\"NUM_WORKERS\"]\n",
        "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n",
        "        config_dict[\"model_yaml_file\"])\n",
        "    cfg.SOLVER.IMS_PER_BATCH = config_dict[\"IMS_PER_BATCH\"]\n",
        "    cfg.SOLVER.BASE_LR = config_dict[\"BASE_LR\"]\n",
        "    cfg.SOLVER.MAX_ITER = config_dict[\"MAX_ITER\"]\n",
        "    cfg.SOLVER.STEPS = []\n",
        "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512\n",
        "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = config_dict[\"NUM_CLASSES\"]\n",
        "\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def train_model(cfg):\n",
        "    \"\"\"[function to train the segment detection model]\n",
        "\n",
        "    Args:\n",
        "            cfg ([type]): [configuration file]\n",
        "    \"\"\"\n",
        "\n",
        "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "    trainer = DefaultTrainer(cfg)\n",
        "    trainer.resume_or_load(resume=True)\n",
        "    trainer.train()\n",
        "\n",
        "\n",
        "def get_predictor(cfg):\n",
        "    \"\"\"[summary]\n",
        "\n",
        "    Args:\n",
        "            cfg ([type]): [configuration file]\n",
        "\n",
        "    Returns:\n",
        "            [type]: [predictor on the trained model]\n",
        "    \"\"\"\n",
        "\n",
        "    cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "    predictor = DefaultPredictor(cfg)\n",
        "\n",
        "    return predictor\n",
        "\n",
        "\n",
        "def run_inference(predictor, metadata, input_json_path):\n",
        "    \"\"\"[function to run inference on test images]\n",
        "\n",
        "    Args:\n",
        "            predictor ([type]): [predictor on the trained images]\n",
        "            metadata ([type]): [metadata of the registered dataset]\n",
        "            input_json_path ([str]): [path to the data json file to run inference on]\n",
        "    \"\"\"\n",
        "\n",
        "    dataset_dicts = get_dataset(input_json_path)\n",
        "    for d in dataset_dicts:\n",
        "        im = cv2.imread(d[\"file_name\"])\n",
        "        outputs = predictor(im)\n",
        "\n",
        "        v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=2, instance_mode=ColorMode.IMAGE_BW)\n",
        "        out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "        cv2_imshow(out.get_image()[:, :, ::-1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI29Mtr_NBdX"
      },
      "source": [
        "    #Make changes to this cell as per your convinience\n",
        "    #the folder structure is:\n",
        "    # parent/\n",
        "    #       |___data/\n",
        "    #       |        |____images/\n",
        "    #       |       |____ annoatation json\n",
        "    #       |___ this notebook\n",
        "    #\n",
        "    \n",
        "    config_dict = {\n",
        "        \"DEVICE\" : \"cuda\", \n",
        "        \"model_out_dir\": \"trained_model\",\n",
        "        \"model_yaml_file\": \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\", # the model file from the model zoo \n",
        "        \"data_base_dir\": \"data\",\n",
        "        \"images_base_dir\": \"data/images\",\n",
        "        \"train_dataset_name\": \"<enter the train dataset name here>\",  # enter the train dataset name \n",
        "        \"test_dataset_name\": (),\n",
        "        \"input_json_path\": \"<the json with dataset annotations in Detectron2 format>\",\n",
        "        \"NUM_WORKERS\": 2,\n",
        "        \"IMS_PER_BATCH\": 2,\n",
        "        \"BASE_LR\": 0.00025,\n",
        "        \"MAX_ITER\": 1000,\n",
        "        \"NUM_CLASSES\": <n+1>, # here n is the length of thing classes list\n",
        "        \"thing_classes\": [str] #list of all classes present in the image to detect. For example: [\"Dog\", \"Cat\", \"Horse\"]\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXd1QzjaNBfF"
      },
      "source": [
        "# Dataset Registery and Initiate the model training\n",
        "\n",
        "# Register the dataset\n",
        "# if already registered load the metadata\n",
        "\n",
        "print(\"\\n Starting dataset Registration\\n\")\n",
        "try:\n",
        "    # for metadata by registering new dataset\n",
        "    metadata = register_dataset(dataset_name=config_dict[\"train_dataset_name\"], input_json_path=os.path.join(config_dict[\"data_base_dir\"], config_dict[\"input_json_path\"]), thing_classes=config_dict[\"thing_classes\"])\n",
        "    print(\"\\n Registered New Dataset\\n\")\n",
        "except:\n",
        "    # for metadata from MetaDataCatalog\n",
        "    metadata = MetadataCatalog.get(config_dict[\"train_dataset_name\"])\n",
        "    print(\"\\n Loaded metadata from already registered dataset\\n\")\n",
        "\n",
        "# create the cfg file for detectron2\n",
        "print(\"\\n Creating config\\n\")\n",
        "config = create_config(config_dict)\n",
        "\n",
        "# create the model directory\n",
        "os.makedirs(config_dict[\"model_out_dir\"], exist_ok='true')\n",
        "\n",
        "# train the model\n",
        "print(\"\\n Initiate Training\\n\")\n",
        "train_model(config)\n",
        "print(\"\\n Training Finished!!!\\n\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xySERivdNBi2"
      },
      "source": [
        "# Get predictions on the Dataset\n",
        "\n",
        "# get the predictor\n",
        "predictor = get_predictor(config)\n",
        "\n",
        "# run inference on the data\n",
        "run_inference(predictor, metadata, input_json_path=os.path.join(config_dict[\"data_base_dir\"], config_dict[\"input_json_path\"]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}